{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewTrefilov/solution-participants-AITrain/blob/master/Detection_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9zaSfEUblpl"
      },
      "source": [
        "<h1 align=\"center\">Detection Baseline</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBLB16nNbUuY"
      },
      "source": [
        "![](https://habrastorage.org/webt/mi/ca/_c/mica_cm-rdj8z3rrztfk4a-k3_q.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7vqTdoad3z3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurvgc6sZhOs"
      },
      "source": [
        "> **ЗАДАЧА:** На основе размеченных фотографий необходимо создать алгоритм детекции следующих объектов: `'Car', 'Human', 'Wagon', 'FacingSwitchL', 'FacingSwitchR', 'FacingSwitchNV', 'TrailingSwitchL', 'TrailingSwitchR', 'TrailingSwitchNV', 'SignalE', 'SignalF'`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2zWVpEJd5Ai"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8RlhId_zD1s"
      },
      "source": [
        "<h1 align=\"center\">Оглавление</h1>\n",
        "\n",
        "- [1. Train/val/test split](#part1) <br>\n",
        "- [2. Подготовим датасет](#part2) <br>\n",
        "    - [2.1 Организация директорий](#part2.1) <br>\n",
        "    - [2.2 Создадим файл с описанием датасетов](#part2.2) <br>\n",
        "    - [2.3 Подготовим изображения и разметку](#part2.3) <br>\n",
        "- [3. Выбор модели](#part3) <br>\n",
        "- [4. Обучение модели](#part4) <br>\n",
        "- [5. Тестирование модели](#part5) <br>\n",
        "- [6. Оценивание](#part5) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2LvsTpBzDsC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC3H__XsdkIZ"
      },
      "source": [
        "<h1 align=\"center\">0. Установка и импорт необходимых библиотек</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmEGckFvdoqw"
      },
      "source": [
        "> **NOTE**: В качестве бейзлайна выбрана модель yolov5, обучать модель будем через командную строку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUza-GCCB3XL"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git > /dev/null\n",
        "!cd yolov5 && git reset --hard  956be8e\n",
        "!cd yolov5 && pip install -r requirements.txt > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T51elqsp_ki3"
      },
      "source": [
        "!git clone https://github.com/sberbank-ai/railway_infrastructure_detection_aij2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdSfgZVBGCmW"
      },
      "source": [
        "import os\n",
        "import yaml\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "\n",
        "\n",
        "# https://github.com/sberbank-ai/railway_infrastructure_detection_aij2021\n",
        "from railway_infrastructure_detection_aij2021.detection.helpers import makedirs, get_yolo_labels, copy_images"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzGZYspoF5_k"
      },
      "source": [
        "<a id='part1'></a>\n",
        "<h1 align=\"center\">1. Train/val/test split</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2nomPRNCbv7"
      },
      "source": [
        "# dataset\n",
        "# !wget https://dsworks.s3pd01.sbercloud.ru/aij2021/AITrain_train/AITrain_train.zip\n",
        "# !unzip -q AITrain_train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkaWTyu0wBld"
      },
      "source": [
        "PATH_TO_BBOXES = 'train_data/bboxes'\n",
        "PATH_TO_IMAGES = 'train_data/images'\n",
        "PATH_TO_SAVE_LABELS = 'train_data/labels'\n",
        "\n",
        "CLASS_NAMES = ['Car', 'Human', 'Wagon', 'FacingSwitchL', 'FacingSwitchR', 'FacingSwitchNV', 'TrailingSwitchL', 'TrailingSwitchR', 'TrailingSwitchNV', 'SignalE', 'SignalF']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmpI3QJXFgPp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e48857-38b2-45a8-aa79-95d5e9567506"
      },
      "source": [
        "random.seed(0)\n",
        "IMAGES_NAME = os.listdir(PATH_TO_IMAGES)\n",
        "random.shuffle(IMAGES_NAME)\n",
        "\n",
        "train_images = IMAGES_NAME[:int(len(IMAGES_NAME)*0.7)]\n",
        "val_images = IMAGES_NAME[int(len(IMAGES_NAME)*0.7): int(len(IMAGES_NAME)*0.85)]\n",
        "test_images = IMAGES_NAME[int(len(IMAGES_NAME)*0.85):]\n",
        "len(train_images), len(val_images), len(test_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5742, 1230, 1231)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF20X4t2Xldo"
      },
      "source": [
        "DATASET_TYPE2IMAGE = {'train': train_images,\n",
        "                      'val': val_images,\n",
        "                      'test': test_images}\n",
        "\n",
        "IMAGE2DATASET_TYPE = {}\n",
        "for key, values in DATASET_TYPE2IMAGE.items():\n",
        "    for file_name in values:\n",
        "        IMAGE2DATASET_TYPE[file_name] = key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpdWB-jQHvzK"
      },
      "source": [
        "<a id='part2'></a>\n",
        "<h1 align=\"center\">2. Подготовим датасет</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxO5tILefqv2"
      },
      "source": [
        "<a id='part2.1'></a>\n",
        "<h2 align=\"center\">2.1 Организация директорий</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJYZVSlyolET"
      },
      "source": [
        "Данные - изображения и разметка для **train**, **val** и **test** выборок, должны быть организованы в соответствии с приведенным ниже примером.  \n",
        "YOLOv5 автоматически находит разметку для каждого изображения, заменяя директорию */ images /* в каждом пути изображения на */ labels /*. \n",
        "Например:  \n",
        "`train_data/images/train/0.jpg  # image`  \n",
        "`train_data/labels/train/0.txt  # label`  \n",
        "   \n",
        "---\n",
        "  \n",
        "*(Поэтому в файле с описанием датасетов достаточно указать только путь до директорий с изображениями, например для **train** выборки `train_data/images/train/`)*\n",
        "\n",
        "\n",
        "```\n",
        "train_data\n",
        "├── images\n",
        "│   ├── test\n",
        "│   ├── train\n",
        "│   └── val\n",
        "└── labels\n",
        "    ├── test\n",
        "    ├── train\n",
        "    └── val\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egflHhxuossD"
      },
      "source": [
        "<a id='part2.2'></a>\n",
        "<h2 align=\"center\">2.2 Создадим файл с описанием датасетов</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1OLwnOfgvdq"
      },
      "source": [
        "```\n",
        "# Пути до датасетов (до директорий с изображениями)\n",
        "train: ../train_data/images/train/\n",
        "val: ../train_data/images/val/\n",
        "test: ../train_data/images/test/\n",
        "\n",
        "# Количество классов\n",
        "nc: 11\n",
        "\n",
        "# Названия классов\n",
        "names: ['Car', 'Human', 'Wagon', 'FacingSwitchL', 'FacingSwitchR', 'FacingSwitchNV', 'TrailingSwitchL', 'TrailingSwitchR', 'TrailingSwitchNV', 'SignalE', 'SignalF']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMl3ZQQO4Ler"
      },
      "source": [
        "aitrain_dataset = [\"train: ../train_data/images/train/\" + \"\\n\",\n",
        "                 \"val: ../train_data/images/val/\" + \"\\n\",\n",
        "                 \"test: ../train_data/images/test/\" + \"\\n\\n\",\n",
        "                 \"nc: 11\" + \"\\n\\n\",\n",
        "                 \"names: ['Car', 'Human', 'Wagon', 'FacingSwitchL', 'FacingSwitchR', 'FacingSwitchNV', 'TrailingSwitchL', 'TrailingSwitchR', 'TrailingSwitchNV', 'SignalE', 'SignalF']\",\n",
        "                ]\n",
        "\n",
        "with open(r'yolov5/data/aitrain_dataset.yaml', 'w') as f:\n",
        "    f.writelines(aitrain_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YngudpHMIzs4"
      },
      "source": [
        "<a id='part2.3'></a>\n",
        "<h2 align=\"center\">2.3 Подготовим изображения и разметку</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouWbzkFxoKvZ"
      },
      "source": [
        "# Создадим нужные директории\n",
        "makedirs(PATH_TO_SAVE_LABELS, PATH_TO_IMAGES, DATASET_TYPE2IMAGE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qulBykmrnZL"
      },
      "source": [
        "# Создадим и сохраним разметку\n",
        "get_yolo_labels(PATH_TO_BBOXES, PATH_TO_SAVE_LABELS, PATH_TO_IMAGES, IMAGE2DATASET_TYPE, CLASS_NAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7jIegiWr9c0",
        "outputId": "fe965f17-0799-443a-a909-59447718cf19"
      },
      "source": [
        "# Скопируем изображения\n",
        "copy_images(IMAGES_NAME, IMAGE2DATASET_TYPE, PATH_TO_IMAGES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8203/8203 [03:52<00:00, 35.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXkPpuKXfx2N"
      },
      "source": [
        "<a id='part3'></a>\n",
        "<h1 align=\"center\">3. Выбор модели</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c0nBQ1hgYoZ"
      },
      "source": [
        "Выберите предварительно обученную модель, с которой можно начать обучение.\n",
        "Предобученные веса загружаются автоматически.\n",
        "Подробнее по [ссылке](https://github.com/ultralytics/yolov5#pretrained-checkpoints)\n",
        "![](https://habrastorage.org/webt/qk/as/y_/qkasy_mlt0kkcih7d7z2wk6li6g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSi2WpUgfbb"
      },
      "source": [
        "<a id='part4'></a>\n",
        "<h1 align=\"center\">4. Обучение модели</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWGjkwHITMtC"
      },
      "source": [
        "По пути `yolov5/data/hyps/hyp.scratch.yaml` - находится файл с настройками гиперпараметров.\n",
        "\n",
        "Поскольку у нас в задаче присутствуют классы, зависящие от левой и правой стороны, поставим вероятность для аугментации fliplr равной нулю:\n",
        "`fliplr: 0.0  # image flip left-right (probability)` , создадим новый файл - `yolov5/data/hyps/hyp_aitrain.yaml`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEnyeXKUXUJ"
      },
      "source": [
        "with open(\"yolov5/data/hyps/hyp.scratch.yaml\", \"r\") as f:\n",
        "    hyps = yaml.safe_load(f)\n",
        "\n",
        "hyps['fliplr'] = 0.0\n",
        "\n",
        "with open(\"yolov5/data/hyps/hyp_aitrain.yaml\", 'w') as f:\n",
        "    yaml.dump(hyps, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys5CK63H4svR",
        "outputId": "1a776f85-511a-4d63-e2e7-d95dca07cb76"
      },
      "source": [
        " # Train YOLOv5m6 on train_dataset for 30 epochs\n",
        "!cd yolov5 && python train.py --img 1280 --batch 8 --epochs 30 --data aitrain_dataset.yaml --weights models/hub/yolov5n6.pt --hyp data/hyps/hyp_aitrain.yaml --name exp_yolov5n6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m6.pt, cfg=, data=aitrain_dataset.yaml, hyp=data/hyps/hyp_aitrain.yaml, epochs=30, batch_size=8, imgsz=1280, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, entity=None, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=100\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v5.0-492-gb0ade48 torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0manchor_t=4.0, box=0.05, cls=0.5, cls_pw=1.0, copy_paste=0.0, degrees=0.0, fl_gamma=0.0, fliplr=0.0, flipud=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, iou_t=0.2, lr0=0.01, lrf=0.2, mixup=0.0, momentum=0.937, mosaic=1.0, obj=1.0, obj_pw=1.0, perspective=0.0, scale=0.5, shear=0.0, translate=0.1, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=11\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Focus                     [3, 48, 3]                    \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  6    629760  models.common.C3                        [192, 192, 6]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   1991808  models.common.Conv                      [384, 576, 3, 2]              \n",
            "  8                -1  2   2327040  models.common.C3                        [576, 576, 2]                 \n",
            "  9                -1  1   3982848  models.common.Conv                      [576, 768, 3, 2]              \n",
            " 10                -1  1   1476864  models.common.SPP                       [768, 768, [3, 5, 7]]         \n",
            " 11                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 12                -1  1    443520  models.common.Conv                      [768, 576, 1, 1]              \n",
            " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 15                -1  2   2658816  models.common.C3                        [1152, 576, 2, False]         \n",
            " 16                -1  1    221952  models.common.Conv                      [576, 384, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 20                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 24                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 27                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
            " 29                -1  2   2437632  models.common.C3                        [768, 576, 2, False]          \n",
            " 30                -1  1   2987136  models.common.Conv                      [576, 576, 3, 2]              \n",
            " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
            " 32                -1  2   4429824  models.common.C3                        [1152, 768, 2, False]         \n",
            " 33  [23, 26, 29, 32]  1     92352  models.yolo.Detect                      [11, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [192, 384, 576, 768]]\n",
            "Model Summary: 503 layers, 35518752 parameters, 35518752 gradients, 51.6 GFLOPs\n",
            "\n",
            "Transferred 644/652 items from yolov5m6.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 107 weight, 111 weight (no decay), 111 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../train_data/labels/train.cache' images and labels... 5742 found, 0 missing, 737 empty, 1 corrupted: 100% 5742/5742 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../train_data/images/train/img_0.4362268919113308.png: duplicate labels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../train_data/labels/val.cache' images and labels... 1230 found, 0 missing, 155 empty, 0 corrupted: 100% 1230/1230 [00:00<?, ?it/s]\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.34, Best Possible Recall (BPR) = 0.8132. Attempting to improve anchors, please wait...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 937 of 31864 labels are < 3 pixels in size.\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 12 anchors on 31842 points...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9986 best possible recall, 5.34 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=12, img_size=1280, metric_all=0.274/0.729-mean/best, past_thr=0.476-mean: 6,6,  11,15,  22,10,  37,16,  21,29,  58,26,  41,53,  93,45,  84,109,  155,77,  225,182,  374,368\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7555: 100% 1000/1000 [00:11<00:00, 86.42it/s]\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9997 best possible recall, 6.01 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=12, img_size=1280, metric_all=0.302/0.755-mean/best, past_thr=0.484-mean: 4,5,  7,9,  19,7,  12,16,  25,12,  39,16,  25,29,  51,26,  72,38,  109,61,  171,124,  301,285\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
            "\n",
            "Image sizes 1280 train, 1280 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp6\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/29     11.5G    0.1112   0.03631   0.03981        34      1280: 100% 718/718 [32:00<00:00,  2.67s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:02<00:00,  1.60s/it]\n",
            "                 all       1230       6912      0.115      0.258       0.11     0.0292\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/29     12.3G   0.09206   0.03044   0.02196        51      1280: 100% 718/718 [32:19<00:00,  2.70s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:58<00:00,  1.54s/it]\n",
            "                 all       1230       6912      0.269      0.262      0.163      0.049\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/29     12.3G   0.08766   0.03103   0.01871        34      1280: 100% 718/718 [31:56<00:00,  2.67s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:57<00:00,  1.52s/it]\n",
            "                 all       1230       6912      0.237      0.278      0.193     0.0572\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/29     12.3G   0.08304   0.03108   0.01648        36      1280: 100% 718/718 [31:36<00:00,  2.64s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:59<00:00,  1.55s/it]\n",
            "                 all       1230       6912      0.302      0.398      0.274     0.0957\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/29     12.3G   0.07799   0.03039   0.01457        40      1280: 100% 718/718 [31:39<00:00,  2.65s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:59<00:00,  1.56s/it]\n",
            "                 all       1230       6912      0.362      0.392      0.312      0.113\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/29     12.3G   0.07474   0.03008   0.01315        39      1280: 100% 718/718 [31:58<00:00,  2.67s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:59<00:00,  1.55s/it]\n",
            "                 all       1230       6912      0.373      0.391      0.315      0.112\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/29     12.3G   0.07332    0.0297   0.01236        45      1280: 100% 718/718 [32:18<00:00,  2.70s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:58<00:00,  1.55s/it]\n",
            "                 all       1230       6912      0.412        0.4      0.358      0.131\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/29     12.3G    0.0708   0.02873   0.01182        49      1280: 100% 718/718 [32:13<00:00,  2.69s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:00<00:00,  1.57s/it]\n",
            "                 all       1230       6912       0.43       0.45      0.389      0.145\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/29     12.3G    0.0694   0.02865   0.01074        52      1280: 100% 718/718 [32:07<00:00,  2.68s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:59<00:00,  1.55s/it]\n",
            "                 all       1230       6912      0.449      0.444      0.398      0.153\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/29     12.3G   0.06788   0.02801   0.01005        59      1280: 100% 718/718 [32:06<00:00,  2.68s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:00<00:00,  1.57s/it]\n",
            "                 all       1230       6912       0.46       0.47      0.414      0.156\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/29     12.3G   0.06686   0.02778   0.00918        75      1280: 100% 718/718 [32:35<00:00,  2.72s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:02<00:00,  1.58s/it]\n",
            "                 all       1230       6912      0.438      0.522      0.421      0.163\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/29     12.3G   0.06572   0.02727  0.009082        24      1280: 100% 718/718 [32:12<00:00,  2.69s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:58<00:00,  1.54s/it]\n",
            "                 all       1230       6912        0.5      0.469      0.439      0.169\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/29     12.3G   0.06502   0.02737  0.008515        39      1280: 100% 718/718 [31:15<00:00,  2.61s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:55<00:00,  1.50s/it]\n",
            "                 all       1230       6912       0.51      0.489      0.449      0.178\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/29     12.3G   0.06335   0.02647  0.008262        33      1280: 100% 718/718 [31:07<00:00,  2.60s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:55<00:00,  1.50s/it]\n",
            "                 all       1230       6912      0.494      0.485      0.441      0.175\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/29     12.3G   0.06275   0.02629  0.007858        56      1280: 100% 718/718 [30:52<00:00,  2.58s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:54<00:00,  1.49s/it]\n",
            "                 all       1230       6912      0.476       0.52      0.454      0.179\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     15/29     12.3G   0.06187   0.02582  0.007507        59      1280: 100% 718/718 [30:49<00:00,  2.58s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:56<00:00,  1.51s/it]\n",
            "                 all       1230       6912      0.546      0.474      0.471      0.189\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     16/29     12.3G   0.06098   0.02528  0.007017        44      1280: 100% 718/718 [30:59<00:00,  2.59s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:55<00:00,  1.50s/it]\n",
            "                 all       1230       6912      0.529      0.506      0.478      0.191\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     17/29     12.3G   0.05975   0.02492  0.006715        45      1280: 100% 718/718 [30:54<00:00,  2.58s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:58<00:00,  1.53s/it]\n",
            "                 all       1230       6912      0.576      0.495      0.481      0.197\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     18/29     12.3G   0.05955   0.02516  0.006405       108      1280: 100% 718/718 [31:12<00:00,  2.61s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:55<00:00,  1.51s/it]\n",
            "                 all       1230       6912      0.578      0.503      0.492      0.199\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     19/29     12.3G   0.05773   0.02465   0.00619        42      1280: 100% 718/718 [30:40<00:00,  2.56s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:56<00:00,  1.52s/it]\n",
            "                 all       1230       6912      0.609      0.506      0.504      0.204\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     20/29     12.3G   0.05741    0.0244  0.005777        39      1280: 100% 718/718 [30:56<00:00,  2.58s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:55<00:00,  1.50s/it]\n",
            "                 all       1230       6912      0.627      0.496      0.517       0.21\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     21/29     12.3G   0.05684   0.02404  0.005473        65      1280: 100% 718/718 [30:57<00:00,  2.59s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:54<00:00,  1.49s/it]\n",
            "                 all       1230       6912      0.628      0.505      0.509      0.206\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     22/29     12.3G   0.05584   0.02389  0.005054        58      1280: 100% 718/718 [30:49<00:00,  2.58s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:51<00:00,  1.45s/it]\n",
            "                 all       1230       6912      0.614      0.519      0.511      0.212\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     23/29     12.3G   0.05533   0.02312   0.00473        44      1280: 100% 718/718 [31:22<00:00,  2.62s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:58<00:00,  1.55s/it]\n",
            "                 all       1230       6912      0.627       0.52      0.517      0.212\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     24/29     12.3G   0.05466   0.02296  0.004675        94      1280: 100% 718/718 [31:32<00:00,  2.64s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:00<00:00,  1.56s/it]\n",
            "                 all       1230       6912      0.608      0.522      0.514      0.214\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     25/29     12.3G   0.05388   0.02258  0.004412        74      1280: 100% 718/718 [32:03<00:00,  2.68s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:58<00:00,  1.54s/it]\n",
            "                 all       1230       6912      0.627      0.528      0.519      0.216\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     26/29     12.3G   0.05327   0.02233  0.004071        40      1280: 100% 718/718 [32:04<00:00,  2.68s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:01<00:00,  1.58s/it]\n",
            "                 all       1230       6912      0.637      0.532      0.531      0.219\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     27/29     12.3G   0.05282   0.02262  0.003926        52      1280: 100% 718/718 [32:14<00:00,  2.69s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:59<00:00,  1.55s/it]\n",
            "                 all       1230       6912      0.653      0.525      0.526      0.215\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     28/29     12.3G   0.05256   0.02213  0.003787        48      1280: 100% 718/718 [32:05<00:00,  2.68s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:59<00:00,  1.56s/it]\n",
            "                 all       1230       6912      0.625      0.531      0.519      0.213\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     29/29     12.3G   0.05206   0.02171  0.003818        70      1280: 100% 718/718 [32:06<00:00,  2.68s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:00<00:00,  1.56s/it]\n",
            "                 all       1230       6912      0.646      0.534       0.53      0.221\n",
            "\n",
            "30 epochs completed in 16.847 hours.\n",
            "Optimizer stripped from runs/train/exp6/weights/last.pt, 71.6MB\n",
            "Optimizer stripped from runs/train/exp6/weights/best.pt, 71.6MB\n",
            "\n",
            "Validating runs/train/exp6/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 396 layers, 35491344 parameters, 0 gradients, 51.5 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [01:59<00:00,  1.55s/it]\n",
            "                 all       1230       6912      0.646      0.532      0.529      0.221\n",
            "                 Car       1230       2270      0.781      0.692      0.742       0.43\n",
            "               Human       1230        486      0.757      0.488      0.563       0.23\n",
            "               Wagon       1230        987      0.803      0.714      0.753      0.454\n",
            "       FacingSwitchL       1230        120      0.651      0.567      0.566      0.226\n",
            "       FacingSwitchR       1230        113      0.718      0.653      0.619      0.238\n",
            "      FacingSwitchNV       1230        182      0.395      0.297      0.235     0.0778\n",
            "     TrailingSwitchL       1230        265      0.724      0.687      0.692      0.267\n",
            "     TrailingSwitchR       1230        151      0.638       0.55      0.543      0.179\n",
            "    TrailingSwitchNV       1230        438       0.39      0.308       0.22       0.06\n",
            "             SignalE       1230        741      0.632      0.412      0.424      0.136\n",
            "             SignalF       1230       1159      0.616      0.489      0.462      0.134\n",
            "Results saved to \u001b[1mruns/train/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS95HSFmRVAF"
      },
      "source": [
        "<a id='part5'></a>\n",
        "<h1 align=\"center\">5. Тестирование модели</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eGrQsEsRdqb"
      },
      "source": [
        "Оценим качество лучших весов модели (которые сохранились по пути `yolov5/runs/train/exp6/weights/best.pt`) на отложенной выборке - `test dataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNzxUsbgQJgE",
        "outputId": "ac9255f6-5dff-4e79-b776-5806f7e5d7e3"
      },
      "source": [
        "!cd yolov5 && python val.py \\\n",
        "               --img 1280 \\\n",
        "               --data aitrain_dataset.yaml\\\n",
        "               --batch-size 16 \\\n",
        "               --task test \\\n",
        "               --weights runs/train/exp_yolov5n6/weights/best.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/aitrain_dataset.yaml, weights=['runs/train/exp6/weights/best.pt'], batch_size=16, imgsz=1280, conf_thres=0.001, iou_thres=0.6, task=test, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False\n",
            "YOLOv5 🚀 v5.0-492-gb0ade48 torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 396 layers, 35491344 parameters, 0 gradients, 51.5 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning '../train_data/labels/test.cache' images and labels... 1231 found, 0 missing, 142 empty, 0 corrupted: 100% 1231/1231 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 77/77 [02:08<00:00,  1.67s/it]\n",
            "                 all       1231       6847       0.62      0.515      0.518      0.217\n",
            "                 Car       1231       2230      0.771       0.71      0.764      0.439\n",
            "               Human       1231        497      0.769      0.539      0.591      0.275\n",
            "               Wagon       1231        909      0.778      0.703      0.755      0.455\n",
            "       FacingSwitchL       1231        133       0.55      0.489      0.479      0.173\n",
            "       FacingSwitchR       1231        133      0.558      0.437      0.451      0.164\n",
            "      FacingSwitchNV       1231        223       0.45      0.327      0.281     0.0996\n",
            "     TrailingSwitchL       1231        303      0.696      0.686      0.663      0.227\n",
            "     TrailingSwitchR       1231        194      0.609      0.515      0.525      0.187\n",
            "    TrailingSwitchNV       1231        453      0.384      0.296      0.241     0.0703\n",
            "             SignalE       1231        768      0.638      0.454      0.461      0.155\n",
            "             SignalF       1231       1004      0.612      0.508      0.483      0.138\n",
            "Speed: 0.5ms pre-process, 22.0ms inference, 2.9ms NMS per image at shape (16, 3, 1280, 1280)\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQP1bsclw6l3"
      },
      "source": [
        "<a id='part6'></a>\n",
        "<h1 align=\"center\">6. Оценивание</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj8xH0q-xBox"
      },
      "source": [
        "> **NOTE**: \n",
        "> - Лучшие веса модели, необходимо поместить в директорию `models/` и назвать `detection_model.pt`.\n",
        "> - Смотрите пример посылки `sample_submission.zip`.\n",
        "> - Проверяющая система будет принимать в качестве ваших предсказаний detection_predictions.json формата COCO, подробнее по [ссылке](https://github.com/cocodataset/cocoapi/blob/master/results/instances_val2014_fakebbox100_results.json) и на официальном [сайте](https://cocodataset.org/#format-data).\n",
        "\n",
        "> - Функция `detection.detection_predict.postprocess` - генерирует нужного вида предсказание для изображения.\n",
        "\n",
        "> - Функция `evaluation.orig_box2coco.make_coco_detection_ann` - конвертирует боксы данные участникам от организаторов к COCO Object Detection формату."
      ]
    }
  ]
}